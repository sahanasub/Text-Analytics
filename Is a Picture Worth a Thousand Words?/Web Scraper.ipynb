{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraper\n",
    "#### Scrape Instagram Image Captions, Likes and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "\n",
    "username='natgeo'\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.instagram.com/'+username+'/?hl=en')\n",
    "Pagelength = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "# Scroll the page to load 100 images :\n",
    "\n",
    "lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "i=0\n",
    "links=[]\n",
    "\n",
    "#might have to load initial images\n",
    "\n",
    "while(match==False):\n",
    "     lastCount = lenOfPage\n",
    "     time.sleep(3)\n",
    "     lenOfPage = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "     i= i+1\n",
    "     source = browser.page_source\n",
    "     data=bs(source, 'html.parser')\n",
    "     body = data.find('body')\n",
    "     script = body.find('span')\n",
    "     for link in script.findAll('a'):\n",
    "          #time.sleep(2)\n",
    "          if re.match(\"/p\", link.get('href')):\n",
    "             links.append('https://www.instagram.com'+link.get('href'))\n",
    "     if i>30 :\n",
    "          match=True\n",
    "           \n",
    "print(links)\n",
    "df_nat = pd.DataFrame(columns = ['display_url','caption','is_video','likes','comments']) \n",
    "        \n",
    "result=pd.DataFrame()\n",
    "for i in range(len(links)):\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        page = urlopen(links[i]).read()\n",
    "        data=bs(page, 'html.parser')\n",
    "        body = data.find('body')\n",
    "        script = body.find('script')\n",
    "        raw = script.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        json_data=json.loads(raw)\n",
    "        posts =json_data['entry_data']['PostPage'][0]['graphql']\n",
    "        posts= json.dumps(posts)\n",
    "        posts = json.loads(posts)\n",
    "        if(posts['shortcode_media']['is_video'] == False):\n",
    "            display_url= posts['shortcode_media']['display_url'] \n",
    "            caption= posts['shortcode_media']['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "            is_video=posts['shortcode_media']['is_video'] \n",
    "            likes= posts['shortcode_media']['edge_media_preview_like']['count'] \n",
    "            comments= posts['shortcode_media']['edge_media_to_parent_comment']['count']\n",
    "            #title= posts['shortcode_media']['title']\n",
    "            df_nat.loc[len(df_nat)] = [display_url,caption,is_video,likes,comments]\n",
    "          \n",
    "       \n",
    "    except:\n",
    "        np.nan\n",
    "\n",
    "df_nat.to_csv(\"C:/Users/Sahana/Downloads/nat_results1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
